---
title: "House Analysis"
author: 'Azadeh Almasi'
date: "2023-05-31"
output:
  pdf_document: default
  html_document: default
---
Install and load required libraries
```{r}
#install.packages(ggplot2)
#install.packages(validate)
#install.packages(car)
library(ggplot2)
library(validate)
library(car)

```


# 1. Organise and clean the data

## 1.1 Subset the data into the specific dataset allocated

```{r}
# Assign any id into the variable SID, for example:
SID <- 2207479                  # This is an example, replace 2101234 with any ID
SIDoffset <- (SID %% 100) + 1    # Your SID mod 100 + 1
load("house-analysis.RDa")
# Now subset the housing data set
# Pick every 100th observation starting from your offset
# Put into your data frame named mydf (you can rename it)
mydf <- house.analysis[seq(from=SIDoffset,to=nrow(house.analysis),by=100),]
```


## 1.2 Data quality analysis
 
Step 1) Installing or Loading required libraries, such as validate or ggplot2 for analysing or modeling sections.
Step 2) Eyeball the data by summary(), str() and table() functions to understand the main points or ideas contained within.
Step 3) Exploring and checking the data more systematically using the validator() function to check for any possible problems such as missing, Invalid  or outliers in the data and check the data types to ensure that each column is the right data type.
Step 4) Checking the result of validator() function numerically and graphically to understand the type and extent of any identified issues for the whole data.
Step 5) Reporting the issues about the data quality that have been uncovered in the data, including their possible implications on analytical or modelling attempts.

Installing or Loading required libraries.

At the start of the script, the essential packages for the analysis or modelling work are installed and loaded. To install the packages, functions like install.packages() and to load them, library() have been used. In this part  validate and ggplot2 have been loaded  because they have been installed before. Validate is a package that contains functions for confirming and cleaning data, whereas ggplot2 is a package that has methods for making visualisations and plots.

Eyeball the data by summary, str and table functions.
```{r}
# Get the main characteristics of the data
summary(mydf)
# Get information about the structure of the data
str(mydf)

```

Exploring and checking the data more systematically.
```{r}
# Define rules for detect issues
mydf.rules <-  validator(
                         OkHeating = is.element(heating,c("autonomous","other")),
                         NonNegprice  = price  > 0,
                         NonNegmq = mq > 0,
                         NonNegfloor = floor > 0,
                         NonNegbathroom = n_bathrooms > 0,
                         NonNegroom = n_rooms >= 0,
                         Str.Terrace = is.factor(has_terrace),
                         Str.Heating = is.factor(heating),
                         Str.Alarm = is.factor(has_alarm),
                         Str.Airconditioning = is.factor(has_air_conditioning),
                         Str.Parking = is.factor(has_parking),
                         Str.Furnished = is.factor(is_furnishe)
                         )

```

Checking the result of validator Numerically and Graphically.
```{r}
mydf.check <- confront(mydf, mydf.rules)
summary(mydf.check)
```

```{r}
barplot(mydf.check, xlab = "")
```


## 1.3 Data cleaning  

List of issues to be fix:

Number 1) There is a negative value in the variable n rooms, which is not acceptable because rooms cannot have negative numbers. This problem must be addressed.
Number 2) There is a spelling error in the heating column, which might lead to misunderstanding or inaccurate analysis. This problem must be corrected by correcting the spelling.
Number 3) There is a 0 value in the mq column, which is not allowed because a property's size can not indeed be zero. This problem must be corrected by changing the zero value with a correct value.
Number 4) The id variable contains no useful information and should be disregarded for the time being.
Number 5) We have five binary(categorical) variables (has_terrace, has_alarm, has_air_conditioning, has_parking, is_furnished) and heating as a categorical variable that have been read in R incorrectly. So, this issue can be fixed in R using as.factor() function.


Imputing the negative variable in n_rooms by sample function
```{r}
# Identify the unreasonable values in the vector
negetive_values <- mydf$n_rooms[mydf$n_rooms < 0]

# Use the sample function to randomly select new values from the rest of the data to replace the unreasonable values
mydf$n_rooms[mydf$n_rooms < 0] <- sample(mydf$n_rooms[mydf$n_rooms >= 0], length(negetive_values))
```

Checking the n_rooms after imputation
```{r}
summary(mydf$n_rooms)
```

Fixing spelling mistake in heating column
```{r}
# Identify the spelling mistake
table(mydf$heating)
```
```{r}
# Replace the mistake value in the heating variable with the correct spelling format
mydf$heating[mydf$heating == "autonamous"] <- "autonomous"
table(mydf$heating)

```

Imputing the zero value(unreasonable) in mq column by median
```{r}

# Compute the median of the data
median_value <- median(mydf$mq, na.rm = TRUE)

# Identify missing or unreasonable values
unreasonable_value <- is.na(mydf$mq) | (mydf$mq == 0)

# Replace missing or unreasonable values with the median value
mydf$mq[unreasonable_value] <- ifelse(unreasonable_value, median_value, mydf$mq)

```
Fixing categorical data formatting using as.factor() function
```{r}
mydf$has_terrace <- as.factor(mydf$has_terrace)
mydf$has_alarm <- as.factor(mydf$has_alarm)
mydf$has_air_conditioning <- as.factor(mydf$has_air_conditioning)
mydf$has_parking <- as.factor(mydf$has_parking)
mydf$is_furnished <- as.factor(mydf$is_furnished)
mydf$heating <- as.factor(mydf$heating)

```


We can then use the summary() function to check that the unreasonable values have been replaced with the median value.

```{r}
summary(mydf$mq)
```
Eyeball the data after cleaning 
```{r}
summary(mydf)
```

All of the noted concerns with the data appear to have been fixed, and the data quality is now good. There are no further issues that have been raised. This implies that the data may now be analysed without fear of mistakes or inaccuracies caused by data difficulties. Before beginning with any study, it is critical to ensure that the data is of excellent quality, as defective data might lead to incorrect or misleading conclusions.

# 2. Exploratory Data Analysis (EDA)

## 2.1 EDA plan

Exploring, describing and visualizing the data is an essential step in data  analysis process, as it helps to get a better understanding of data and identify any patterns or trends that may be present. 

The approach to explore each variable depends on their type. So, we use different approaches to explore our categorical and numerical variables.

Step 1) Load the data into R.

Step2) Get a numerical summary of the variables to explore the essential characteristics of the data using summary() and table() functions.
In addition, we can use functions such as str(), head(), and tail() to get a sense of the structure and content of the data.

Step 3) Graphically summary of the variables.
Examine the data for patterns or trends, and keep an eye out for outliers or oddities.
The histogram, which depicts the frequency and distribution of values in the data, can be used to provide a graphical summary of numerical variables.
A bar plot or box plot can also be used to provide a graphical overview of categorical variables.

Step 4) Discovering the correlation and relationships between pairs of variables using functions such as cor().

Step 5) Visualizing the relationship between variables. Using scatter plot, plot or box plot.

Step 6) Summarizing the findings.
 
 
## 2.2 EDA and summary of results  

Step 1) Step 1 have been done in Organize and clean the data section, part 1.1.

Step 2) Get a numerical summary of the variables
```{r}
# Getting the main characteristics of the data
summary(mydf)
# Getting the structure of the data
str(mydf)
```

Eyeball the first and last six of the data
```{r}
# Getting the first six rows of the data
head(mydf)
# Getting the last six rows of the data
tail(mydf)
```

Checking the categorical variables using table() function
```{r}
table(mydf$heating)
table(mydf$has_terrace)
table(mydf$has_alarm)
table(mydf$has_air_conditioning)
table(mydf$has_parking)
table(mydf$is_furnished)

```


Step 3) Graphically summary of variables

Using the histogram for the continuous variables
```{r}
ggplot(mydf, aes(x=price)) + geom_histogram(bins = 10, color="darkgrey", fill="grey") + theme_classic() + ggtitle("Histogram of the Price variable")

ggplot(mydf, aes(x=mq)) + geom_histogram(bins = 10, color="darkgrey", fill="grey") + theme_classic() + ggtitle("Histogram of the mq variable")

```
Additionally, a bar chart can be used to show the frequency of the discrete variables such as floor, n_rooms and n_bathrooms.
```{r}
ggplot(data = mydf, aes(floor)) +
  geom_bar(color="darkgrey", fill = "grey") +
  labs(title = "Frequency of floor variable") 


ggplot(data = mydf, aes(n_rooms)) +
  geom_bar(color="darkgrey", fill = "grey") +
  labs(title = "Frequency of n_rooms variable") 


ggplot(data = mydf, aes(n_bathrooms)) +
  geom_bar(color="darkgrey", fill = "grey") +
  labs(title = "Frequency of n_bathrooms variable") 

```


Using the shapiro.test() method to check whether or not our continuous data is normally distributed.
```{r}
shapiro.test(mydf$price)
shapiro.test(mydf$mq)
```
According to the p-value (p-value < 0.05), the data is not normally distributed, transformation or normalization techniques such as log transformation, square root transformation, the inverse transformation or the Box-Cox transformation can be used to make our data more normally distributed.


Using log transformation to make the data more normally distributed.
```{r}
# Apply the log transformation to the data
price_log <- log(mydf$price)
# Plot the transformed data
hist(price_log, main = "log-transformed price variable")


# Apply the log transformation to the data
mq_log <- log(mydf$mq)
# Plot the transformed data
hist(mq_log, main = "log-transformed mq variable")

```

Checking that the log transformation makes the data more normally distributed or not, using shapiro.test() function.
```{r}
shapiro.test(price_log)
shapiro.test(mq_log)
```
Log transformation, as seen by the histogram and p-value, causes the data to be more normally distributed.
 
For graphically summary of categorical variables, bar chart or pie chart can be used.
```{r}
ggplot(mydf)+ geom_bar(aes(x=has_terrace), fill="grey", color="darkgrey") +
  labs(title = "Bar plot of has_terrace variable") +
  theme_classic()

ggplot(mydf)+ geom_bar(aes(x=heating), fill="grey", color="darkgrey") +
  labs(title = "Bar plot of heating variable") +
  theme_classic()

ggplot(mydf)+ geom_bar(aes(x=has_alarm), fill="grey", color="darkgrey") +
  labs(title = "Bar plot of has_alarm variable") +
  theme_classic()

ggplot(mydf)+ geom_bar(aes(x=has_air_conditioning), fill="grey", color="darkgrey") +
  labs(title = "Bar plot of has_air_conditioning variable") +
  theme_classic()

ggplot(mydf)+ geom_bar(aes(x=has_parking), fill="grey", color="darkgrey") +
  labs(title = "Bar plot of has_parking variable") +
  theme_classic()

ggplot(mydf)+ geom_bar(aes(x=is_furnished), fill="grey", color="darkgrey") +
  labs(title = "Bar plot of is_furnished variable") +
  theme_classic()

```

Step 4) Discovering the correlation and relationships between pairs of variables using functions such as cor() based on the research questions.

### Discovering relationship between two numerical variables
```{r}
cor(price_log, mq_log)
```

A correlation value of one represents a perfect positive connection, whereas one shows a perfect negative correlation. A correlation value of 0 shows that there is no relationship.
A correlation value of 0.4 shows a moderate positive connection between the two variables, implying that when one measure rises, the other is likely to rise as well.

Using the cor.test() function, to see if there is a significant relationship between the two variables.
our null and alternative hypothesis are:
$H_0: \rho=0$
$H_1:\rho \ne 0$
```{r}
cor.test(price_log, mq_log)
```

We reject the null hypothesis since the p_value is less than 0.05. As a result, these two variables are correlated.

### Discovering the relationship between two numerical variables
Statistical tests such as chi-square or Fisher's exact tests can be used to test the relationship between two categorical variables.
To decide which one of these tests should be used, the value of the variables using the table() function should be checked. if there was not a value less than 5 chi-square should be used but if there was a value bigger than 5 Fisher's exact can be used.

The null hypothesis that we are testing for each two pairs of variables is:
$H_0$: The is_furnished and heating are independent.
The alternative hypothesis is:
$H_1$: There is a relationship between them.

Checking the value of the variable by table() function
```{r}
table(mydf$is_furnished, mydf$heating)
```
As the variable's value is not less than 5 chisq.test() function can be used.

```{r}
chisq.test(table(mydf$is_furnished, mydf$heating))
```
Since p-value is bigger than 0.05 we accept $H_0$. So, there is no relationship between these two variables.


```{r}
table(mydf$is_furnished, mydf$has_terrace)
```
As the variable's values are not less than 5 we can use chisq.test() function.
```{r}
chisq.test(table(mydf$is_furnished, mydf$has_terrace))
```
According to the p-value there is not a relationship between them.



Check the values of variables to determine the type of test to use.
```{r}
table(mydf$is_furnished, mydf$has_alarm)
```

Since one of the expected frequencies is < 5 fisher.test() function should be used.
```{r}
# Fisher's exact test for and has_alarm
fisher.test(table(mydf$is_furnished, mydf$has_alarm))
```
According to the p-value there is no relationship.


```{r}
table(mydf$is_furnished, mydf$has_air_conditioning)
```

```{r}
chisq.test(table(mydf$is_furnished,mydf$has_air_conditioning))

```
Since p-value is less than 0.05 so they are correlated.


```{r}
table(mydf$is_furnished, mydf$has_parking)
```

```{r}
fisher.test(table(mydf$is_furnished, mydf$has_parking))
```
They are unrelated due to the fact that the p-value is greater than 0.05.

Step 5) Visualizing the relationship between variables considering the research questions. Using scatter plot, plot, mosaic plot or box plot.

Research question 1) According to the property price research question, our dependent variable is continuous and explanatory variables are a mix of continuous, discrete and categorical.


Using scatter plot to visualize the relationship between two continuous variables.
```{r}
ggplot(data=mydf, aes(x=price_log, y=mq_log)) + geom_point() + theme_classic() +
  ggtitle("Scatter Plot of price vs mq")
```

As we can see there is a moderate positive correlation between the two variables, meaning that as total square meters of the property increases, the price of the property is likely to also increase. 

Using scatter plot to visualize the relationship between a continuous variable and a discrete variable.
```{r}
ggplot(data=mydf, aes(x=floor, y=price_log)) + geom_point() + theme_classic() +
ggtitle("Scatter Plot of price vs floor")

ggplot(data=mydf, aes(x=n_rooms, y=price_log)) + geom_point() + theme_classic() +
ggtitle("Scatter Plot of price vs n_rooms")

ggplot(data=mydf, aes(x=n_bathrooms, y=price_log)) + geom_point() + theme_classic() +
ggtitle("Scatter Plot of price vs n_bathrooms")

```

Using box plot to visualize the relationship between a continuous variable and categorical variables.
```{r}
ggplot(data = mydf, aes(x=heating, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of price by heating")

ggplot(data = mydf, aes(x=has_terrace, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of price by has_terrace")

ggplot(data = mydf, aes(x=has_alarm, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of price by has_alarm")

ggplot(data = mydf, aes(x=has_air_conditioning, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of price by has_air_conditioning")

ggplot(data = mydf, aes(x=has_terrace, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of price by has_parking")

ggplot(data = mydf, aes(x=is_furnished, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of price by is_furnished")
```
                 
                  
Research question 2) Then according to the likelihood of a property being furnished research question our dependent variable is binary(categorical) and our explanatory variables are a mix of continuous and categorical.

Exploring the relationship between is_furnished as a categorical variable and continues variables. 
```{r}
ggplot(data = mydf, aes(x=is_furnished, y=price_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of is_furnished by price")

ggplot(data = mydf, aes(x=is_furnished, y=mq_log)) + geom_boxplot() + theme_classic() + ggtitle("Box Plot of is_furnished by price")

```

Furthermore, mosaic plots can be used to explore the relationship and distribution of two categorical variables.
```{r}
mosaicplot(mydf$is_furnished~mydf$heating, main = "Mosaic plot of furnishing status by heating", ylab="Heating", xlab="Furnishing Status")

mosaicplot(mydf$is_furnished~mydf$has_air_conditioning, main = "Mosaic plot of furnishing status by air conditioning situation", ylab="air conditioning Status", xlab="Furnishing Status")

mosaicplot(mydf$is_furnished~mydf$has_parking, main = "Mosaic plot of furnishing status by parking situation", ylab="Parking Status", xlab="Furnishing Status")

mosaicplot(mydf$is_furnished~mydf$has_terrace, main = "Mosaic plot of furnishing status by terrace situation", ylab="Terrace Status", xlab="Furnishing Status")

mosaicplot(mydf$is_furnished~mydf$has_alarm, main = "Mosaic plot of furnishing status by alarm situation", ylab="Alarm Status", xlab="Furnishing Status")
```


## 2.3 Additional insights and issues

In section 2.2 I have explained insights and issues in each part but in summary, besides outliers, unreasonable values, and incorrect structures that have been explored in the data cleaning part, there are further issues that have been uncovered in the Exploratory Data Analysis section.

According to the histogram of the continuous variables and the result of the Shapiro test, continuous variables did not follow a bell-shaped curve shape. So, as they were not normal, log transformation can be used to make our data more normally distributed.


# 3. Modelling

## 3.1 Explain your analysis plan

According to the research question as our dependent variable is continuous two modelling approaches can be used in this step. ANCOVA and Multiple Regression Model.

Multiple Regression)
the outline for conducting multiple linear regression in R:

Step 1) Load and import the data into R. Eyeball structure and content of the data.

Step 2) Prepare the data for analysis by cleaning it and fixing issues.

Step 3) Explore the data to get a sense of the variable distribution and relationship.

Step 4) Fit the maximal multiple regression model using the lm() function and get a summary.


Step 5) Calculate the minimum adequate model using the step() function and get a summary.

Step 6) Plot the final model and Interpret the results using the coefficients and p-values.


ANCOVA) 
The outline for conducting an analysis of covariance (ANCOVA) in R:

Step 1) Begin by importing and cleaning the data.

Step 2) Next, fit an ANCOVA model using the aov() function.

Step 3) Analyze the model fit by examining the ANOVA table, which can be obtained using the summary() function.

Step 4) Finally, interpret the result and visualize the results of the ANCOVA using plots to better understand the relationship between the variables.



## 3.2 Build a model for property price

Our dependent variable is Continuous so we according to the dependent variable will select one of modelling approaches such as t-test, ANOVA, ANCOVA, Linear Regression or Multiple Regression models.

In this question, as our dependent variable is continuous and our independent variables are a mix of continuous and categorical types we can use *Multiple Regression* or *ANCOVA*.

### Multiple Regression

Maximal model
```{r}
maximal.model <- lm(price ~ mq + floor+n_rooms+n_bathrooms+has_terrace+has_alarm+heating+has_air_conditioning+has_parking+is_furnished , data=mydf)

summary(maximal.model)
```
Now we can delete the insignificant independent variables manually step by step to have our *minimal adequate model* but instead we can use step() function to do it.
So, now we will use the step function to get our *minimal adequate model*, we do this instead of going step by step manually.

```{r}
min.model <- step(maximal.model)
```
As we can see, the step function produces a long result. So, we're seeking for the last section, which presents the *minimal suitable model* where the step process ends.

Let's look at this model in more details
```{r}
min.model.lm <- lm(formula =price ~ mq + floor + n_rooms + n_bathrooms + heating + has_air_conditioning
, data=mydf)
summary(min.model.lm)
```
A p-value of less than 0.05 is regarded as statistically significant. As a consequence, while has_air_conditioning1, n_rooms and floor are not significant, the other three independent variables are significant predictors of the dependent variable.
As a result, our *minimal adequate model* is as follows:

$$price = 8.95 + 0.46 \times mq + 0.35 \times n_bathrooms + 0.21 \times heating$$

```{r}
plot(min.model)
```

As we can see in Residuals vs Fitted values plot there is larger scatter with larger fitted values.
In addition, in Q-Q plot there is a S shape or Banana shape pattern. So, the errors are not normally distributed.
As e results the plots for the residuals point to some issues that can be fix with log transformation. I will do it as an improvement in 3.4 part.



## 3.3 Critique model using relevant diagnostics

The following results can be used to evaluate model fit and interpret results after fitting a multiple regression model in R.

Model characteristics)
A smaller p-value (less than 0.05) indicates that the explanatory variables are significantly related to the response variable. Therefore, mq, n-bathrooms and heating variables are significant.

Goodness of fit) 
The R-squared value of 0.2764 indicates that the independent variables explain approximately 28% of the variance in the dependent variable (price). This indicates that the model has a moderate fit to the data.

The F-statistic and p-value (57.03 and p < 2.2e-16) indicate that the model is statistically significant.

Graphical diagnostics)
As I have explained in Residuals vs Fitted values plot there is larger scatter with larger fitted values(heteroscedasticity).
In addition, in Q-Q plot there is a S shape or Banana shape pattern. So, the errors are not normally distributed.
As e results the plots for the residuals  point to some issues that can be fix with log transformation I will do it as an improvement in 3.4 part.


## 3.4 Suggest improvements to your model

Suggestion 1) Using Musing Multiple Regression, as there is heteroscedasticity there is a S shape or Banana shape pattern in the Normal Q-Q plot. Transforming the dependent variable and the variable that was not normally distributed by log transformation can help address the problems identified in the previous step.

Suggestion 2) Because the dependent variable is continuous and the independent variables are a mix of continuous and categorical, ANCOVA can be check for as an alternative approach in this case.

### Improvements in Multiple Regression model

Maximal model:
```{r}
# Fit the model
model.improved <- lm(log(price) ~ log(mq)+floor+n_rooms+n_bathrooms+has_terrace+has_alarm+heating+has_air_conditioning+has_parking+is_furnished , data=mydf)

summary(model.improved)
```
Now we can delete the insignificant independent variables manually by step to have our *minimal adequate model*. But instead, step() function can be used to do it automatically.
```{r}
min.model.improved <- step(model.improved)
```
As we can see the result of step function is long so what we are looking for is the last one, which proposes the *minimal adequate model* where the step process stops.

Let's look at this model in more details
```{r}
final.model <- lm(log(price) ~ log(mq) + n_bathrooms + heating, data=mydf)
summary(final.model)
```
A p-value less than 0.05 is considered to be statistically significant. Therefore according to the result, all three independent variables are significant predictors of the dependent variable.

So our *minimal adequate model* is:

 $$log(price) = 8.95 + 0.46 \times log(mq) + 0.35 \times n_bathrooms + 0.21 \times heating$$

```{r}
plot(final.model)
```

In Residuals vs Fitted plot there is no larger scatter with larger fitted values.
In addition, in the Normal Q-Q plot, there is no S shape or Banana shape pattern. It is a straight line so the errors are normally distributed.
In the end, the points in plot 3 are not distributed in a triangular shape.
As e result, the plots for the residuals don't point to any issues.


### ANCOVA as an alternative approach
```{r}
# Fit the model
ancova.mydf <-lm(price_log~mq_log+mydf$floor+mydf$n_rooms+mydf$n_bathrooms+mydf$has_terrace+mydf$has_alarm+mydf$heating+mydf$has_air_conditioning+mydf$has_parking+mydf$is_furnished)
summary(ancova.mydf)
```

```{r}
# Perform ANCOVA
Anova(ancova.mydf,  type = "III")
```
Just mq_log, n_bathrooms and heating have a significant effect on the dependent variable.

```{r}
plot(ancova.mydf)
```

As e result, the plots do not point to any issues. Because there is no heteroscedasticity and plot 2 is not S or Banana shape.

# 4. Extension work

## 4.1 Model the likelihood of a property being furnished (using the is_furnished variable provided).

The likelihood of property being furnished is a binary (categorical) dependent variable so a good approach to model this relationship is *Logistic Regression*. 

Making sure that our dependent variable is indeed being treated as a factor.
```{r}
str(mydf$is_furnished)
```

Using table() function to confirm that our dependent variable only has 2 values.
```{r}
table(mydf$is_furnished)
```


Maximal model without interactions:
```{r}
furnished.max<-glm(is_furnished ~ price + mq + floor + n_rooms + n_bathrooms + has_terrace + has_alarm + heating + has_air_conditioning + has_parking, family=binomial, data = mydf )

summary(furnished.max)
```
According to the result just has_air_conditioning1 is significant because it's p-value is less than 0.05.


In addition, we can do some model selection using step function to see if this can be improved.
```{r}
furnished.min <- step(furnished.max)
```

Looking at the result model in more detail:
```{r}
furnished.min.glm <- glm(formula = is_furnished ~ has_air_conditioning, family = binomial, 
    data = mydf)
summary(furnished.min.glm )
```

In the result we can see has_air_conditioning1 is significant because it's p-value is less than 0.05.

### Odd ratios and Odds
We use the following steps to compute the odd ratios:
```{r}
# Using the model : furnished.max
# Extract the coefficients of the model using coef()
# Exponentiate the coefficients
exp(coef(furnished.max))
```
The odds ratios can tell us what influence a change in the dependent variable has on the odds of being furnished, in this model and data. 

Interpreting the result of odd ratios:
According to the result, we can summarize that mq_log, floor, n_rooms, n_bathrooms,has_terrace1, has_alarm1 and has_air_conditioning1 increase furnished likelihood slightly.
has_parking1 increases the furnished likelihood more by a factor of 2.48.
Additionally, price_log and heatingother decrease furnished likelihood by a factor of 0.86 and  0.69.

Also The model can be used to predict the probability of being furnished using:
```{r}
# Predict the probability of infection from the model
mydf$pfurnished<-predict(furnished.max, type="response")
head(mydf)
```
